{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "solver = \"minion\"\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "model_classifier = False\n",
    "type_folder = \"/single_problem_custom\"\n",
    "dir_logs = \"../ML_logs/rf_all/\"+solver+type_folder\n",
    "files = os.listdir(dir_logs)\n",
    "\n",
    "ground_truth = \"../Dataset/results_on_off_\"+solver+\".csv\"\n",
    "result_df = pd.read_csv(ground_truth)\n",
    "result_df.drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "max_time = result_df['SolverTotalTime'].max() + result_df['SavileRowTotalTime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dict()\n",
    "tot_time_without_tab = 0.0\n",
    "tot_time_with_tab = 0.0\n",
    "for prob in result_df[\"Problem\"].unique():\n",
    "    baseline = result_df.query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "    if len(baseline) == 0:\n",
    "        continue\n",
    "    baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "    tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "    if len(tab2) == 0:\n",
    "        continue\n",
    "    tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "    baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "    tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "    if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "        if model_classifier:\n",
    "            targets[prob] = {'Target': 0, 'Score': 0.0, 'Tab_t': 3600, 'Base_t': 3600}\n",
    "        else:\n",
    "            continue  # drop row\n",
    "    if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "        clipped = np.clip(tab2_time, 0, 3600)\n",
    "        targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "    elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "        clipped = np.clip(baseline_time, 0, 3600)\n",
    "        targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 3600, 'Base_t': clipped}\n",
    "    elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "        if model_classifier:\n",
    "            label = 1 if tab2_time < baseline_time else 0\n",
    "            clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "            clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "            targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "        else:\n",
    "            continue # Skip small differences\n",
    "    elif baselineTimeOut==0 and tab2TimeOut==0:\n",
    "        label = 1 if tab2_time < baseline_time else 0\n",
    "        clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "        clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "        targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "target_df['Problem'] = target_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gnn_flag = False\n",
    "name = dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "dfs = dict()\n",
    "if (name == \"instance\" or name == \"problem\") and not gnn_flag:\n",
    "    problems = []\n",
    "    for file in files:\n",
    "        split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "        if split not in dfs.keys():\n",
    "            dfs[split] = pd.DataFrame()\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "        problems.append([test_problems, train_problems])\n",
    "        dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "        dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "    for split in dfs.keys():\n",
    "        dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"single\" and not gnn_flag:\n",
    "    for file in files:\n",
    "        split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "        if split not in dfs.keys():\n",
    "            dfs[split] = pd.DataFrame()\n",
    "        complete_df = pd.DataFrame()\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test, all_train = pickle.load(pkl_f)\n",
    "        dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "        dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "        dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"leave\" and not gnn_flag:\n",
    "    complete_df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test = pickle.load(pkl_f)[0]\n",
    "        complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "    df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "    dfs['0.1'] = df\n",
    "elif name == \"instance\" and gnn_flag:\n",
    "    for file in files:\n",
    "        split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "        if split not in dfs.keys():\n",
    "            dfs[split] = pd.DataFrame()\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test = pickle.load(pkl_f)[0]\n",
    "        all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "        all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "        dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "    for split in dfs.keys():\n",
    "        dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"leave\" and gnn_flag:\n",
    "    complete_df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test = pickle.load(pkl_f)[0]\n",
    "        all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "        if len(all_test[\"prediction\"][0][0])==1:\n",
    "            all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "        complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "    df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "    dfs['0.1'] = df\n",
    "elif name == \"problem\" and gnn_flag:\n",
    "    for file in files:\n",
    "        split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "        if split not in dfs.keys():\n",
    "            dfs[split] = pd.DataFrame()\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test = pickle.load(pkl_f)[0]\n",
    "        all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "        all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "        dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "    for split in dfs.keys():\n",
    "        dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"single\" and gnn_flag:\n",
    "    for file in files:\n",
    "        split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "        if split not in dfs.keys():\n",
    "            dfs[split] = pd.DataFrame()\n",
    "        complete_df = pd.DataFrame()\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test = pickle.load(pkl_f)[0]\n",
    "        all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "        if len(all_test[\"prediction\"][0][0])==1:\n",
    "            all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "        dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "        dfs[split].reset_index().drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if name == \"instance\" or name == \"problem\":\n",
    "    for df_key in dfs.keys():\n",
    "        complete_df = dfs[df_key]\n",
    "        df = pd.DataFrame()\n",
    "        num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "        for prob in complete_df['Problem'].unique():\n",
    "            prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "            for train_test in prob_df['train_test'].unique():\n",
    "                a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "                tmp_df = a[num_cols].mean().to_frame().T\n",
    "                tmp_df[\"Problem\"] = prob\n",
    "                tmp_df[\"train_test\"] = train_test\n",
    "                tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "                df = pd.concat([df, tmp_df])\n",
    "        dfs[df_key] = df.reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"single\" or name==\"leave\":\n",
    "    for df_key in dfs.keys():\n",
    "        print(len(dfs[df_key]))\n",
    "        dfs[df_key] = dfs[df_key].groupby([\"Problem\", \"train_test\"]).agg({\n",
    "            \"saved_v_best\": 'mean',\n",
    "            \"saved_class\": 'mean',\n",
    "            \"saved_heur\": 'mean',\n",
    "            \"tot_best\": 'mean',\n",
    "            \"tot_class\": 'mean',\n",
    "            \"tot_heur\": 'mean',\n",
    "            \"tot_no_tab\": 'mean'\n",
    "        })\n",
    "        dfs[df_key] = dfs[df_key].reset_index()\n",
    "        print(len(dfs[df_key]))\n",
    "if gnn_flag and \"instance\" in dfs.keys():\n",
    "    dfs['0.3'] = dfs[\"instance\"]\n",
    "elif gnn_flag and \"problem\" in dfs.keys():\n",
    "    dfs['0.3'] = dfs[\"problem\"]\n",
    "for df_key in dfs.keys():\n",
    "    sums_by_split = dfs[df_key].query(\"train_test=='test'\").drop([\"Problem\", \"train_test\"], axis = 1).sum()\n",
    "    if sums_by_split[\"saved_heur\"]>0:\n",
    "        sums_by_split[\"gap\"] = 100*(sums_by_split[\"tot_class\"]-sums_by_split[\"tot_heur\"])/(sums_by_split[\"tot_best\"]-sums_by_split[\"tot_heur\"])\n",
    "    else:\n",
    "        sums_by_split[\"gap\"] = 100*(sums_by_split[\"tot_class\"]-sums_by_split[\"tot_no_tab\"])/(sums_by_split[\"tot_best\"]-sums_by_split[\"tot_no_tab\"])\n",
    "    print(\"Split: \", df_key, \"\\n\", sums_by_split, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_key = \"0.1\"\n",
    "def get_metrics(tab_act_num=1):\n",
    "    target_df[\"pred\"] = -1\n",
    "    if name in [\"leave\", \"single\"]:\n",
    "        probs_stats = pd.DataFrame()\n",
    "        for _, row in dfs[best_key].query(\"train_test == 'test'\").iterrows():\n",
    "            prob_stats_dict = {\"fp\": [0], \"fp_time\": [0.0], \"fn\": [0], \"fn_time\": [0.0], \n",
    "                               \"no_diff\": [0], \"no_diff_time\": [0.0], \"Problem\": row[\"Problem\"], \n",
    "                               \"tp\": [0], \"tp_time\": [0.0], \"tn\": [0], \"tn_time\": [0.0], \"tp_heur\": [0.0], \n",
    "                               \"fp_heur\": [0.0], \"tn_heur\": [0.0], \"fn_heur\": [0.0], \"disagreement_time\": [0.0], \n",
    "                               \"agreement_time\": [0.0]}\n",
    "            for label_instance in row[\"prediction\"]:\n",
    "                instance = label_instance[4:]\n",
    "                target_df.loc[instance, \"pred\"] = 1 if label_instance[:3]==\"pos\" else 0\n",
    "                stats = target_df.query(\"Problem == '\"+ instance+ \"'\")\n",
    "                if len(stats) <1:\n",
    "                    continue\n",
    "                fp_flag = stats[\"pred\"].values[0]==1 and stats[\"Target\"].values[0] == 0\n",
    "                fn_flag = stats[\"pred\"].values[0]==0 and stats[\"Target\"].values[0] == 1\n",
    "                tp_flag = stats[\"pred\"].values[0]==1 and stats[\"Target\"].values[0] == 1\n",
    "                tn_flag = stats[\"pred\"].values[0]==0 and stats[\"Target\"].values[0] == 0\n",
    "                if tab_act_num != -1:\n",
    "                    tab_activated = result_df.query(\"Problem == '\"+instance+\"'\")[\"TabulationActivated\"+str(tab_act_num)].dropna()\n",
    "                else:\n",
    "                    tab_activated = result_df.query(\"Problem == '\"+instance+\"'\")[\"TabulationActivated\"].dropna()\n",
    "                if len(tab_activated) <1:\n",
    "                    continue\n",
    "                tab_activated = [1 if x>0 else 0 for x in tab_activated]\n",
    "                tp_heur = tab_activated[0] == 1 and stats[\"pred\"].values[0]==1\n",
    "                fp_heur = tab_activated[0] == 0 and stats[\"pred\"].values[0]==1\n",
    "                tn_heur = tab_activated[0] == 0 and stats[\"pred\"].values[0]==0\n",
    "                fn_heur = tab_activated[0] == 1 and stats[\"pred\"].values[0]==0\n",
    "                if fp_flag and stats[\"similar\"].values[0] == 0:\n",
    "                    prob_stats_dict[\"fp\"][0] += 1\n",
    "                    prob_stats_dict[\"fp_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif fn_flag and stats[\"similar\"].values[0] == 0:\n",
    "                    prob_stats_dict[\"fn\"][0] += 1\n",
    "                    prob_stats_dict[\"fn_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif (fp_flag or fn_flag) and stats[\"similar\"].values[0] == 1:\n",
    "                    prob_stats_dict[\"no_diff\"][0] += 1\n",
    "                    prob_stats_dict[\"no_diff_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif tp_flag:\n",
    "                    prob_stats_dict[\"tp\"][0] += 1\n",
    "                    prob_stats_dict[\"tp_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif tn_flag:\n",
    "                    prob_stats_dict[\"tn\"][0] += 1\n",
    "                    prob_stats_dict[\"tn_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                if fp_heur:\n",
    "                    prob_stats_dict[\"fp_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"disagreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "                elif fn_heur:\n",
    "                    prob_stats_dict[\"fn_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"disagreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "                elif tp_heur:\n",
    "                    prob_stats_dict[\"tp_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"agreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "                elif tn_heur:\n",
    "                    prob_stats_dict[\"tn_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"agreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "            prob_stats_dict[\"precision\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fp\"][0]+0.000001)\n",
    "            prob_stats_dict[\"recall\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fn\"][0]+0.000001)\n",
    "            prob_stats_dict[\"f1\"]=2*prob_stats_dict[\"recall\"]*prob_stats_dict[\"precision\"]/(prob_stats_dict[\"recall\"]+prob_stats_dict[\"precision\"]+0.000001)\n",
    "            probs_stats = pd.concat([probs_stats, pd.DataFrame.from_dict(prob_stats_dict)])\n",
    "        probs_stats = probs_stats.reset_index()\n",
    "        probs_stats.drop([\"Problem\"], axis=1).describe()\n",
    "    elif name in [\"problem\", \"instance\"]:\n",
    "        probs_stats = pd.DataFrame()\n",
    "        done = []\n",
    "        for _, row in dfs[best_key].query(\"train_test == 'test'\").iterrows():\n",
    "            curr_prob = row[\"Problem\"].split(\"_\")[0]\n",
    "            prob_stats_dict = {\"fp\": [0], \"fp_time\": [0.0], \"fn\": [0], \"fn_time\": [0.0], \n",
    "                               \"no_diff\": [0], \"no_diff_time\": [0.0], \"tp\": [0], \"tp_time\": [0.0],\n",
    "                               \"tn\": [0], \"tn_time\": [0.0], \"tp_heur\": [0.0], \n",
    "                               \"fp_heur\": [0.0], \"tn_heur\": [0.0], \"fn_heur\": [0.0], \"disagreement_time\": [0.0], \n",
    "                               \"agreement_time\": [0.0]}\n",
    "            fp_flag = row[\"saved_v_best\"]==0 and row[\"saved_heur\"]<0 and row[\"saved_class\"] < row[\"saved_heur\"]\n",
    "            fn_flag = row[\"saved_v_best\"]>0 and row[\"saved_heur\"]>0 and row[\"saved_class\"] <= 0\n",
    "            tp_flag = row[\"saved_v_best\"]>0 and row[\"saved_heur\"]>0 and row[\"saved_class\"] > 0\n",
    "            tn_flag = row[\"saved_v_best\"]==0 and row[\"saved_heur\"]<0 and row[\"saved_class\"] <= 0\n",
    "            if tab_act_num == -1:\n",
    "                tab_activated = result_df.query(\"Problem == '\"+row[\"Problem\"]+\"' and Policy=='2'\")[\"TabulationActivated\"].dropna()\n",
    "            else:\n",
    "                tab_activated = result_df.query(\"Problem == '\"+row[\"Problem\"]+\"' and Policy=='2'\")[\"TabulationActivated\"+str(tab_act_num)].dropna()\n",
    "            if len(tab_activated) <1:\n",
    "                continue\n",
    "            tab_activated = [1 if x>0 else 0 for x in tab_activated]\n",
    "            tp_heur = tab_activated[0] == 1 and (tp_flag or fp_flag)\n",
    "            fp_heur = tab_activated[0] == 0 and (tp_flag or fp_flag)\n",
    "            tn_heur = tab_activated[0] == 0 and (tn_flag or fn_flag)\n",
    "            fn_heur = tab_activated[0] == 1 and (tn_flag or fn_flag)\n",
    "            if fp_flag:\n",
    "                prob_stats_dict[\"fp\"][0] += 1\n",
    "            elif fn_flag:\n",
    "                prob_stats_dict[\"fn\"][0] += 1\n",
    "            elif tp_flag:\n",
    "                prob_stats_dict[\"tp\"][0] += 1\n",
    "            elif tn_flag:\n",
    "                prob_stats_dict[\"tn\"][0] += 1\n",
    "            if fp_heur:\n",
    "                prob_stats_dict[\"fp_heur\"][0] += 1\n",
    "                prob_stats_dict[\"disagreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            elif fn_heur:\n",
    "                prob_stats_dict[\"fn_heur\"][0] += 1\n",
    "                prob_stats_dict[\"disagreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            elif tp_heur:\n",
    "                prob_stats_dict[\"tp_heur\"][0] += 1\n",
    "                prob_stats_dict[\"agreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            elif tn_heur:\n",
    "                prob_stats_dict[\"tn_heur\"][0] += 1\n",
    "                prob_stats_dict[\"agreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            prob_stats_dict[\"precision\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fp\"][0]+0.000001)\n",
    "            prob_stats_dict[\"recall\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fn\"][0]+0.000001)\n",
    "            prob_stats_dict[\"f1\"]=2*prob_stats_dict[\"recall\"]*prob_stats_dict[\"precision\"]/(prob_stats_dict[\"recall\"]+prob_stats_dict[\"precision\"]+0.000001)\n",
    "            probs_stats = pd.concat([probs_stats, pd.DataFrame.from_dict(prob_stats_dict)])\n",
    "        probs_stats = probs_stats.reset_index()\n",
    "    return probs_stats, target_df[target_df[\"pred\"]!=-1].copy()\n",
    "\n",
    "if \"TabulationActivated1\" in result_df.columns:\n",
    "    agreement_dict = dict()\n",
    "    disagreement_dict = dict()\n",
    "    for i in [1, 2, 3, 4, 5]:\n",
    "        probs_stats_tmp, target_df_tmp = get_metrics(tab_act_num=i)\n",
    "        agreement_dict[i] = probs_stats_tmp[\"agreement_time\"].sum()\n",
    "        disagreement_dict[i] = probs_stats_tmp[\"disagreement_time\"].sum()\n",
    "        if i == 1:\n",
    "            probs_stats = probs_stats_tmp\n",
    "            if not model_classifier:\n",
    "                target_df = target_df_tmp\n",
    "else:\n",
    "    probs_stats_tmp, target_df_tmp = get_metrics(tab_act_num=-1)\n",
    "    probs_stats = probs_stats_tmp\n",
    "    if not model_classifier:\n",
    "        target_df = target_df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name in [\"leave\", \"single\"]:\n",
    "    print(\"Total false positives: \", probs_stats[\"fp\"].sum(), \", Cost of fp: \", np.abs(probs_stats[\"fp_time\"]).sum())\n",
    "    print(\"Total false negatives: \", probs_stats[\"fn\"].sum(), \", Cost of fn: \", np.abs(probs_stats[\"fn_time\"]).sum())\n",
    "    print(\"Total no diff: \", probs_stats[\"no_diff\"].sum(), \", Cost of no_diff: \", np.abs(probs_stats[\"no_diff_time\"]).sum())\n",
    "    print(\"Total true positives: \", probs_stats[\"tp\"].sum(), \", Saved with tp: \", np.abs(probs_stats[\"tp_time\"]).sum())\n",
    "    print(\"Total true negatives: \", probs_stats[\"tn\"].sum(), \", Time of tn: \", np.abs(probs_stats[\"tn_time\"]).sum())\n",
    "    print(\"Mean precision: \", probs_stats[\"precision\"].mean())\n",
    "    print(\"Mean recall: \", probs_stats[\"recall\"].mean())\n",
    "    print(\"Mean f1: \", probs_stats[\"f1\"].mean())\n",
    "    print(\"Agreement heuristics-ML: \", probs_stats[\"tp_heur\"].sum()+probs_stats[\"tn_heur\"].sum())\n",
    "    print(\"Disagreement heuristics-ML: \", probs_stats[\"fn_heur\"].sum()+probs_stats[\"fp_heur\"].sum())\n",
    "    print(\"Disagreement time saved: \", probs_stats[\"disagreement_time\"].sum())\n",
    "    print(\"Agreement time saved: \", probs_stats[\"agreement_time\"].sum())\n",
    "    probs_stats.query(\"fp>0 or fn>0 or no_diff>0\").drop([\"index\"], axis=1).reset_index().drop([\"index\"], axis=1)\n",
    "else:\n",
    "    print(\"Total false positives: \", probs_stats[\"fp\"].sum())\n",
    "    print(\"Total false negatives: \", probs_stats[\"fn\"].sum())\n",
    "    print(\"Total true positives: \", probs_stats[\"tp\"].sum())\n",
    "    print(\"Total true negatives: \", probs_stats[\"tn\"].sum())\n",
    "    print(\"Mean precision: \", probs_stats[\"precision\"].mean())\n",
    "    print(\"Mean recall: \", probs_stats[\"recall\"].mean())\n",
    "    print(\"Mean f1: \", probs_stats[\"f1\"].mean())\n",
    "    print(\"Agreement heuristics-ML: \", probs_stats[\"tp_heur\"].sum()+probs_stats[\"tn_heur\"].sum())\n",
    "    print(\"Disagreement heuristics-ML: \", probs_stats[\"fn_heur\"].sum()+probs_stats[\"fp_heur\"].sum())\n",
    "    print(\"Disagreement time saved: \", probs_stats[\"disagreement_time\"].sum())\n",
    "    print(\"Agreement time saved: \", probs_stats[\"agreement_time\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"single\" or name==\"leave\":\n",
    "    seconds = 60\n",
    "    if not gnn_flag:\n",
    "        titles = {\"saved_class\": \"Proportion of time saved for each problem with random forest classifier\", \n",
    "                 \"saved_v_best\": \"Proportion of time saved for each problem with virtual best classifier\"}\n",
    "    else:\n",
    "        titles = {\"saved_class\": \"Proportion of time saved for each problem with Graph Neural Network\", \n",
    "                 \"saved_v_best\": \"Proportion of time saved for each problem with virtual best classifier\"}\n",
    "    saved_thresh = \", when the time saved is more than \"+str(seconds)+\" seconds.\"\n",
    "    total_thresh = \", when the total time is more than \"+str(seconds)+\" seconds.\"\n",
    "    for col in [\"saved_class\", \"saved_v_best\"]:\n",
    "        for ext in [\".\", \"thresh\"]:\n",
    "            if ext == \"thresh\" and col.split(\"_\")[0] == \"saved\":\n",
    "                title = titles[col] + saved_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \"thresh\" and col.split(\"_\")[0] == \"tot\":\n",
    "                title = titles[col] + total_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \".\":\n",
    "                title = titles[col] + \".\"\n",
    "                thresh = 0\n",
    "            a = dfs[best_key].query(\"train_test == 'test'\")[[\"Problem\", col, \"tot_heur\", \"tot_no_tab\"]]\n",
    "            a = a[a[col]>thresh]\n",
    "            b = a[\"tot_heur\"]  # no_tab\"]\n",
    "            a[col] = b-(a[\"tot_no_tab\"]-a[col])  # saved with respect to the heuristics\n",
    "            a = a[[\"Problem\", col]]\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.bar(np.arange(0, len(a[\"Problem\"]), 1), a[col]/b*100)\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Problem\")\n",
    "            plt.ylabel(\"Proportion of time saved with respect to always using the heuristics.\")  # the baseline.\")\n",
    "            plt.xticks(ticks=np.arange(0, len(a[\"Problem\"]), 1), labels=a[\"Problem\"]);\n",
    "            plt.savefig(col+\"_\"+ext+\"_heur.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"single\" or name==\"leave\":\n",
    "    seconds = 60\n",
    "    if not gnn_flag:\n",
    "        titles = {\"saved_class\": \"Proportion of time saved for each problem with random forest classifier\", \n",
    "                 \"saved_v_best\": \"Proportion of time saved for each problem with virtual best classifier\"}\n",
    "    else:\n",
    "        titles = {\"saved_class\": \"Proportion of time saved for each problem with Graph Neural Network\", \n",
    "                 \"saved_v_best\": \"Proportion of time saved for each problem with virtual best classifier\"}\n",
    "    saved_thresh = \", when the time saved is more than \"+str(seconds)+\" seconds.\"\n",
    "    total_thresh = \", when the total time is more than \"+str(seconds)+\" seconds.\"\n",
    "    for col in [\"saved_class\", \"saved_v_best\"]:\n",
    "        for ext in [\".\", \"thresh\"]:\n",
    "            if ext == \"thresh\" and col.split(\"_\")[0] == \"saved\":\n",
    "                title = titles[col] + saved_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \"thresh\" and col.split(\"_\")[0] == \"tot\":\n",
    "                title = titles[col] + total_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \".\":\n",
    "                title = titles[col] + \".\"\n",
    "                thresh = 0\n",
    "            a = dfs[best_key].query(\"train_test == 'test'\")[[\"Problem\", col, \"tot_no_tab\"]]\n",
    "            if thresh == 0:\n",
    "                a = a[abs(a[col])>1000]\n",
    "            else:\n",
    "                a = a[a[col]>thresh]\n",
    "            b = a[\"tot_no_tab\"]\n",
    "            a = a[[\"Problem\", col]]\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.bar(np.arange(0, len(a[\"Problem\"]), 1), a[col]/b*100)\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Problem\")\n",
    "            plt.ylabel(\"Proportion of time saved with respect to always using the baseline.\")\n",
    "            plt.xticks(ticks=np.arange(0, len(a[\"Problem\"]), 1), labels=a[\"Problem\"]);\n",
    "            plt.savefig(col+\"_\"+ext+\"_base.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"single\" or name==\"leave\":\n",
    "    seconds = 60\n",
    "    if not gnn_flag:\n",
    "        titles = {\"tot_heur\": \"Total time for each problem when always using the heuristics\", \n",
    "                 \"tot_no_tab\": \"Total time for each problem without tabulation.\"}\n",
    "    else:\n",
    "        titles = {\"tot_heur\": \"Total time for each problem when always using the heuristics\", \n",
    "                 \"tot_no_tab\": \"Total time for each problem without tabulation.\"}\n",
    "    saved_thresh = \", when the time saved is more than \"+str(seconds)+\" seconds.\"\n",
    "    total_thresh = \", when the total time is more than \"+str(seconds)+\" seconds.\"\n",
    "    for col in [\"tot_heur\", \"tot_no_tab\"]:\n",
    "        for ext in [\".\", \"thresh\"]:\n",
    "            if ext == \"thresh\" and col.split(\"_\")[0] == \"saved\":\n",
    "                title = titles[col] + saved_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \"thresh\" and col.split(\"_\")[0] == \"tot\":\n",
    "                title = titles[col] + total_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \".\":\n",
    "                title = titles[col] + \".\"\n",
    "                thresh = 0\n",
    "            a = dfs[best_key].query(\"train_test == 'test'\")[[\"Problem\", col]]\n",
    "            a = a[a[col]>thresh]\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.bar(np.arange(0, len(a[\"Problem\"]), 1), np.log(a[col]))\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Problem\")\n",
    "            plt.ylabel(\"Time (s), logaritmic scale\")\n",
    "            plt.xticks(ticks=np.arange(0, len(a[\"Problem\"]), 1), labels=a[\"Problem\"]);\n",
    "            plt.savefig(col+\"_\"+ext+\".png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df_key in [best_key]:  # dfs.keys():\n",
    "    print(\"Split: \", df_key, \"\\n\")\n",
    "    a = dfs[df_key].query(\"train_test=='test'\").drop([\"Problem\", \"train_test\"], axis = 1).sum()\n",
    "    b = a[['saved_v_best', 'saved_class', 'saved_heur']]\n",
    "    c = a[['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab']]\n",
    "    b.plot(kind='bar')\n",
    "    plt.title(\"Total time saved with solver: \" + solver[0].upper()+solver[1:])\n",
    "    plt.xticks(ticks=np.arange(0,len(b),1), labels=['saved_v_best', 'saved_class', 'saved_heur'])\n",
    "    plt.savefig(\"time_saved.png\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    c.plot(kind='bar')\n",
    "    plt.title(\"Total time with solver: \" + solver[0].upper()+solver[1:])\n",
    "    plt.xticks(ticks=np.arange(0,len(c),1), labels=['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab'])\n",
    "    plt.savefig(\"time_total.png\", bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if name == \"instance\":\n",
    "    from utils import calculateSpeedups_2, _2dp, _gm\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    df = dfs[best_key].copy()\n",
    "    list_probs = df.query(\"train_test=='test'\").Problem.apply(lambda x: x.split(\"_\")[0]).unique()\n",
    "    dict_probs = dict()\n",
    "    for prob in list_probs:\n",
    "        dict_probs[prob.lower()] = prob\n",
    "    list_probs = sorted([x.lower() for x in list_probs])\n",
    "    list_probs = [dict_probs[x] for x in list_probs]\n",
    "    for i, prob in enumerate(list_probs[:28]):\n",
    "        ax = fig.add_subplot(7, 4, i+1)\n",
    "        tmp_df = df[df.Problem.str.contains(prob)].query(\"train_test=='test'\")\n",
    "        df_speedups = calculateSpeedups_2(tmp_df, False, \"saved_class\")\n",
    "        ax.scatter(tmp_df['tot_heur'], tmp_df['tot_class'], c=\"black\", marker=\"x\");\n",
    "        ax.set_yscale('symlog')\n",
    "        ax.set_xscale('symlog')\n",
    "        ax.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "        ax.set_xlim(0, 4805)\n",
    "        ax.set_ylim(0, 4805)\n",
    "        ax.set_title(prob[0].upper()+prob[1:]);\n",
    "        facet_speedups = df_speedups[\"speedup\"].to_numpy()\n",
    "        gm = _2dp(_gm(facet_speedups))\n",
    "        _lim=(1,1.5 * 3600)\n",
    "        ax.text(0.5 * _lim[1], 2 * _lim[0], s=f\"s = {gm}\", ha='right')\n",
    "    fig.supylabel(\"Total time in log scale with random forest classifier (s)\");\n",
    "    fig.supxlabel(\"Total time in log scale when always using the heuristics (s)\");\n",
    "    plt.savefig(solver+\"_multi_scatter_1.png\")\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    df = dfs[best_key]\n",
    "    for i, prob in enumerate(list_probs[28:52]):\n",
    "        ax = fig.add_subplot(6, 4, i+1)\n",
    "        tmp_df = df[df.Problem.str.contains(prob)].query(\"train_test=='test'\")\n",
    "        df_speedups = calculateSpeedups_2(tmp_df, False, \"saved_class\")\n",
    "        ax.scatter(tmp_df['tot_heur'], tmp_df['tot_class'], c=\"black\", marker=\"x\");\n",
    "        ax.set_yscale('symlog')\n",
    "        ax.set_xscale('symlog')\n",
    "        ax.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "        ax.set_xlim(0, 4805)\n",
    "        ax.set_ylim(0, 4805)\n",
    "        ax.set_title(prob[0].upper()+prob[1:]);\n",
    "        facet_speedups = df_speedups[\"speedup\"].to_numpy()\n",
    "        gm = _2dp(_gm(facet_speedups))\n",
    "        _lim=(1,1.5 * 3600)\n",
    "        ax.text(0.5 * _lim[1], 2 * _lim[0], s=f\"s = {gm}\", ha='right')\n",
    "    fig.supylabel(\"Total time in log scale with random forest classifier (s)\");\n",
    "    fig.supxlabel(\"Total time in log scale when always using the heuristics (s)\");\n",
    "    plt.savefig(solver+\"_multi_scatter_2.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df_key in [best_key]:  # dfs.keys():\n",
    "    df = dfs[df_key]\n",
    "    plt.figure(figsize=(8, 8));\n",
    "    ax = plt.gca()\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        max_time = int(max(df.query(\"train_test=='test'\")['tot_best'].max(), df.query(\"train_test=='test'\")['tot_heur'].max()))\n",
    "    ax.scatter(df.query(\"train_test=='test'\")['tot_heur'], df.query(\"train_test=='test'\")['tot_best'], c=\"black\", marker=\"x\");\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.set_xscale('symlog')\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "    else:\n",
    "        plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "    if name != 'leave':\n",
    "        plt.title(\"Total time for problems in test set with split=\"+df_key);\n",
    "        plt.ylabel(\"Total time of virtual best classifier (s) with split=\"+df_key);\n",
    "        plt.xlabel(\"Total time when always using the heuristics (s) with split=\"+df_key);\n",
    "    else:\n",
    "        plt.title(\"Total time for problems in test set.\");\n",
    "        plt.ylabel(\"Total time of virtual best classifier (s).\");\n",
    "        plt.xlabel(\"Total time when always using the heuristics (s).\");\n",
    "    plt.tight_layout();\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        plt.xlim(left=0, right=max_time);\n",
    "        plt.ylim(bottom=0, top=max_time);\n",
    "    else:\n",
    "        plt.xlim(left=0, right=4800);\n",
    "        plt.ylim(bottom=0, top=4800);\n",
    "    plt.savefig(\"best_heuristics.jpg\", bbox_inches = 'tight')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df_key in [best_key]:  # dfs.keys():\n",
    "    df = dfs[df_key]\n",
    "    plt.figure(figsize=(8, 8));\n",
    "    ax = plt.gca()\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        max_time = int(max(df.query(\"train_test=='test'\")['tot_heur'].max(), df.query(\"train_test=='test'\")['tot_class'].max()))\n",
    "    ax.scatter(df.query(\"train_test=='test'\")['tot_heur'], df.query(\"train_test=='test'\")['tot_class'], c=\"black\", marker=\"x\");\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.set_xscale('symlog')\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "    else:\n",
    "        plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "    if name != 'leave':\n",
    "        plt.title(\"Total time for problems in test set with split=\"+df_key);\n",
    "        plt.ylabel(\"Total time of random forest classifier (s) with split=\"+df_key);\n",
    "        plt.xlabel(\"Total time when always using the heuristics (s) with split=\"+df_key);\n",
    "    else:\n",
    "        plt.title(\"Total time for problems in test set.\");\n",
    "        plt.ylabel(\"Total time of random forest classifier (s).\");\n",
    "        plt.xlabel(\"Total time when always using the heuristics (s).\");\n",
    "    plt.tight_layout();\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        plt.xlim(left=0, right=max_time);\n",
    "        plt.ylim(bottom=0, top=max_time);\n",
    "    else:\n",
    "        plt.xlim(left=0, right=4800);\n",
    "        plt.ylim(bottom=0, top=4800);\n",
    "    plt.savefig(\"class_heuristics_test.jpg\", bbox_inches = 'tight')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not gnn_flag:\n",
    "    for df_key in [best_key]:  # dfs.keys():\n",
    "        df = dfs[df_key]\n",
    "        plt.figure(figsize=(8, 8));\n",
    "        ax = plt.gca()\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            max_time = int(max(df.query(\"train_test=='train'\")['tot_heur'].max(), df.query(\"train_test=='train'\")['tot_class'].max()))\n",
    "        ax.scatter(df.query(\"train_test=='train'\")['tot_heur'], df.query(\"train_test=='train'\")['tot_class'], c=\"black\", marker=\"x\");\n",
    "        ax.set_yscale('symlog')\n",
    "        ax.set_xscale('symlog')\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "        else:\n",
    "            plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "        if name != 'leave':\n",
    "            plt.title(\"Total time for problems in train set with split=\"+df_key);\n",
    "            plt.ylabel(\"Total time of random forest classifier (s) with split=\"+df_key);\n",
    "            plt.xlabel(\"Total time when always using the heuristics (s) with split=\"+df_key);\n",
    "        else:\n",
    "            plt.title(\"Total time for problems in train set.\");\n",
    "            plt.ylabel(\"Total time of random forest classifier (s).\");\n",
    "            plt.xlabel(\"Total time when always using the heuristics (s).\");\n",
    "        plt.tight_layout();\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            plt.xlim(left=0, right=max_time);\n",
    "            plt.ylim(bottom=0, top=max_time);\n",
    "        else:\n",
    "            plt.xlim(left=0, right=4800);\n",
    "            plt.ylim(bottom=0, top=4800);\n",
    "        plt.savefig(\"class_heuristics_train.jpg\", bbox_inches = 'tight')\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_solv = \"minion\"\n",
    "type_folder = \"/single_problem_custom\"\n",
    "\n",
    "def run_setting(tmp_solv, type_folder):\n",
    "    df_solvs = dict()\n",
    "    df_solvs_class = dict()\n",
    "    for tmp_set in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]:\n",
    "        tmp_dir_logs = \"../ML_logs/\"+tmp_set+\"/\"+tmp_solv+type_folder\n",
    "        tmp_files = os.listdir(tmp_dir_logs)\n",
    "        tmp_ground_truth = \"../Dataset/results_on_off_\"+tmp_solv+\".csv\"\n",
    "        df_solvs[tmp_set] = pd.read_csv(tmp_ground_truth)\n",
    "        df_solvs[tmp_set].drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "        targets = dict()\n",
    "        tot_time_without_tab = 0.0\n",
    "        tot_time_with_tab = 0.0\n",
    "        for prob in df_solvs[tmp_set][\"Problem\"].unique():\n",
    "            baseline = df_solvs[tmp_set].query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "            if len(baseline) == 0:\n",
    "                continue\n",
    "            baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "            tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "            if len(tab2) == 0:\n",
    "                continue\n",
    "            tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "            baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "                if model_classifier:\n",
    "                    targets[prob] = {'Target': 0, 'Score': 0.0, 'Tab_t': 3600, 'Base_t': 3600}\n",
    "                else:\n",
    "                    continue  # drop row\n",
    "            if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "                clipped = np.clip(tab2_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "                clipped = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 3600, 'Base_t': clipped}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "                if model_classifier:\n",
    "                    label = 1 if tab2_time < baseline_time else 0\n",
    "                    clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                    clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                    targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "                else:\n",
    "                    continue # Skip small differences\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0:\n",
    "                label = 1 if tab2_time < baseline_time else 0\n",
    "                clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "        target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "        target_df['Problem'] = target_df.index\n",
    "        target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)\n",
    "        df_solvs[tmp_set] = target_df.copy()\n",
    "        name = tmp_dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "        dfs = dict()\n",
    "        if (name == \"instance\" or name == \"problem\") and not gnn_flag:\n",
    "            problems = []\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "                problems.append([test_problems, train_problems])\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "            for split in dfs.keys():\n",
    "                dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"single\" and not gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                complete_df = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test, all_train = pickle.load(pkl_f)\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "                dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"leave\" and not gnn_flag:\n",
    "            complete_df = pd.DataFrame()\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "            df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "            dfs['0.1'] = df\n",
    "        elif name == \"instance\" and gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "            for split in dfs.keys():\n",
    "                dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"leave\" and gnn_flag:\n",
    "            complete_df = pd.DataFrame()\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                if len(all_test[\"prediction\"][0][0])==1:\n",
    "                    all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "                complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "            df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "            dfs['0.1'] = df\n",
    "        elif name == \"problem\" and gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "            for split in dfs.keys():\n",
    "                dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"single\" and gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                complete_df = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                if len(all_test[\"prediction\"][0][0])==1:\n",
    "                    all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "                dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        if name == \"instance\" or name == \"problem\":\n",
    "            for df_key in dfs.keys():\n",
    "                complete_df = dfs[df_key]\n",
    "                df = pd.DataFrame()\n",
    "                num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "                for prob in complete_df['Problem'].unique():\n",
    "                    prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "                    for train_test in prob_df['train_test'].unique():\n",
    "                        a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "                        tmp_df = a[num_cols].mean().to_frame().T\n",
    "                        tmp_df[\"Problem\"] = prob\n",
    "                        tmp_df[\"train_test\"] = train_test\n",
    "                        tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "                        df = pd.concat([df, tmp_df])\n",
    "                dfs[df_key] = df.reset_index().drop([\"index\"], axis=1)\n",
    "                dfs[df_key][\"saved_class_v_heur\"] = dfs[df_key][\"saved_class\"]-dfs[df_key][\"saved_heur\"]\n",
    "            if gnn_flag and \"instance\" in dfs.keys():\n",
    "                dfs['0.3'] = dfs[\"instance\"]\n",
    "            elif gnn_flag and \"problem\" in dfs.keys():\n",
    "                dfs['0.3'] = dfs[\"problem\"]\n",
    "        elif name == \"single\" or name==\"leave\":\n",
    "            for df_key in dfs.keys():\n",
    "                dfs[df_key] = dfs[df_key].groupby([\"Problem\", \"train_test\"]).agg({\n",
    "                    \"saved_v_best\": 'mean',\n",
    "                    \"saved_class\": 'mean',\n",
    "                    \"saved_heur\": 'mean',\n",
    "                    \"tot_best\": 'mean',\n",
    "                    \"tot_class\": 'mean',\n",
    "                    \"tot_heur\": 'mean',\n",
    "                    \"tot_no_tab\": 'mean'\n",
    "                })\n",
    "                dfs[df_key] = dfs[df_key].reset_index()\n",
    "                dfs[df_key][\"saved_class_v_heur\"] = dfs[df_key][\"saved_class\"]-dfs[df_key][\"saved_heur\"]\n",
    "        if type_folder == \"/single_problem_custom\":\n",
    "            keys_dict = {\"minion\": \"0.1\", \"kissat\": \"0.4\", \"kissat_mdd\": \"0.1\", \"chuffed\": \"0.1\"}  # problem class\n",
    "        if type_folder == \"/leave_one_out_custom\":\n",
    "            keys_dict = {\"minion\": \"0.1\", \"kissat\": \"0.1\", \"kissat_mdd\": \"0.1\", \"chuffed\": \"0.1\"}  # leave_one_out\n",
    "        if type_folder == \"/instance_custom\":\n",
    "            keys_dict = {\"minion\": \"0.3\", \"kissat\": \"0.5\", \"kissat_mdd\": \"0.2\", \"chuffed\": \"0.3\"}  # instance\n",
    "        df_solvs_class[tmp_set] = dfs[keys_dict[tmp_solv]].copy()\n",
    "    return df_solvs, df_solvs_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of different settings and subsets of features for a given solver\n",
    "tmp_solv = \"kissat\"\n",
    "dfs_boxs = pd.DataFrame()\n",
    "for setting in [\"single_problem\", \"leave_one_out\"]:\n",
    "    type_folder = \"/\"+setting+\"_custom\"\n",
    "    _, df_solvs_class = run_setting(tmp_solv, type_folder)\n",
    "    \n",
    "    df_list = []\n",
    "    for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]:\n",
    "        df_solvs_class[subset_t][\"Problem\"] = df_solvs_class[subset_t].apply(lambda x: x[\"Problem\"].split(\"_\")[0], axis=1)\n",
    "        df_solvs_class[subset_t] = df_solvs_class[subset_t].groupby([\"Problem\", \"train_test\"]).agg(\"mean\", numeric_only=True).reset_index()\n",
    "        \n",
    "        if setting == \"single_problem\":\n",
    "                df_solvs_class[subset_t][\"saved_s_best\"] = 0\n",
    "                for prob in df_solvs_class[subset_t].Problem.unique():\n",
    "                    a = df_solvs_class[subset_t].query(\"train_test=='train' and Problem=='\"+prob+\"'\")\n",
    "                    mask = df_solvs_class[subset_t][\"Problem\"].str.match(prob) & df_solvs_class[subset_t][\"train_test\"].str.match(\"train\")\n",
    "                    df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].iloc[0]<=0 else a[\"saved_heur\"]\n",
    "                    b = df_solvs_class[subset_t].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                    mask = df_solvs_class[subset_t][\"Problem\"].str.match(prob) & df_solvs_class[subset_t][\"train_test\"].str.match(\"test\")\n",
    "                    df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].iloc[0]<=0 else b[\"saved_heur\"]\n",
    "                    df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[subset_t].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"leave_one_out\":\n",
    "            df_solvs_class[subset_t][\"saved_s_best\"] = 0\n",
    "            for prob in df_solvs_class[subset_t].Problem.unique():\n",
    "                a = df_solvs_class[subset_t].query(\"train_test=='test' and Problem!='\"+prob+\"'\")\n",
    "                b = df_solvs_class[subset_t].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                mask = df_solvs_class[subset_t][\"Problem\"].str.match(prob) & df_solvs_class[subset_t][\"train_test\"].str.match(\"test\")\n",
    "                df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].sum()<=0 else b[\"saved_heur\"]\n",
    "                df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[subset_t].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"instance\":\n",
    "            p_list = set([prob[0] for prob in df_solvs_class[subset_t][\"Problem\"].str.split(\"_\")])\n",
    "            prob_sum = dict()\n",
    "            train_sum = dict()\n",
    "            for prob in p_list:\n",
    "                prob_sum[prob] = df_solvs_class[subset_t][df_solvs_class[subset_t][\"Problem\"].str.contains(prob+\"_\")].query(\"train_test=='test'\").sum()\n",
    "                train_sum[prob] = df_solvs_class[subset_t][df_solvs_class[subset_t][\"Problem\"].str.contains(prob+\"_\")].query(\"train_test=='train'\").sum()\n",
    "            df_solvs_class[subset_t] = pd.DataFrame(prob_sum).transpose().drop([\"Problem\", \"prediction\"], axis=1)\n",
    "            train_sum = pd.DataFrame(train_sum).transpose().drop([\"Problem\", \"prediction\"], axis=1)\n",
    "            df_solvs_class[subset_t] = df_solvs_class[subset_t].reset_index()\n",
    "            df_solvs_class[subset_t][\"Problem\"] = df_solvs_class[subset_t][\"index\"]\n",
    "            df_solvs_class[subset_t][\"saved_s_best\"] = prob_sum[prob][\"saved_class\"] if train_sum[\"saved_heur\"].sum()<=0 else prob_sum[prob][\"saved_class\"]-prob_sum[prob][\"saved_heur\"]\n",
    "    \n",
    "        if \"prediction\" in df_solvs_class[subset_t].columns:\n",
    "            df_list.append(df_solvs_class[subset_t].query(\"train_test=='test'\").drop([\"prediction\", \"train_test\"], axis=1).copy().assign(Trial=subset_t))\n",
    "        else:\n",
    "            df_list.append(df_solvs_class[subset_t].query(\"train_test=='test'\").drop([\"train_test\"], axis=1).copy().assign(Trial=subset_t))\n",
    "\n",
    "        if setting == \"instance\":\n",
    "            df_list[-1] = df_list[-1].drop([\"index\"], axis=1).copy().assign(Trial=subset_t)\n",
    "            cols = [x for x in df_list[-1].columns if x != \"Trial\" and x!= \"Problem\"]\n",
    "            df_list[-1][cols] = df_list[-1][cols].astype(float)\n",
    "            \n",
    "        df_list[-1][\"tot_class_log\"] = np.log(df_list[-1][\"tot_class\"])\n",
    "    \n",
    "        df_list[-1][\"tot_class - \"+subset_t] = df_list[-1][\"tot_class\"]\n",
    "        df_list[-1][\"tot_class_log - \"+subset_t] = df_list[-1][\"tot_class_log\"]\n",
    "        df_list[-1][\"saved_class - \"+subset_t] = df_list[-1][\"saved_class\"]\n",
    "        df_list[-1][\"saved_class_v_heur - \"+subset_t] = df_list[-1][\"saved_class_v_heur\"]\n",
    "        df_list[-1][\"saved_s_best - \"+subset_t] = df_list[-1][\"saved_s_best\"]\n",
    "    \n",
    "        df_list[-1] = df_list[-1][[\"tot_class - \"+subset_t, \"tot_class_log - \"+subset_t, \"saved_class - \"+subset_t, \"saved_class_v_heur - \"+subset_t, \"saved_s_best - \"+subset_t, \"Problem\"]]\n",
    "    \n",
    "    dfs_boxs_tmp = df_list[0].merge(df_list[-1], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[1], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[2], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.drop([\"Problem\"], axis=1)\n",
    "    \n",
    "    dfs_boxs_tmp[\"Setting\"] = \" \".join([x[0].upper()+x[1:] for x in setting.split(\"_\")])\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"By Instance\" if x==\"Instance\" else x)\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"Per Problem Class\" if x==\"Single Problem\" else x)\n",
    "    \n",
    "    dfs_boxs = pd.concat([dfs_boxs, dfs_boxs_tmp])\n",
    "    \n",
    "    print(\"Finished \" + setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "threshold = 0\n",
    "\n",
    "a = dfs_boxs[[\"Setting\"]+[\"tot_class - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"tot_class - RF subset\"] = a[\"tot_class - rf_subset\"]\n",
    "a[\"tot_class - DT subset\"] = a[\"tot_class - dt_subset\"]\n",
    "a[\"tot_class - DT all\"] = a[\"tot_class - dt_all\"]\n",
    "a[\"tot_class - RF all\"] = a[\"tot_class - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = False);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Total time (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"total_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "\n",
    "a = dfs_boxs[[\"Setting\"]+[\"tot_class_log - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"tot_class_log - RF subset\"] = a[\"tot_class_log - rf_subset\"]\n",
    "a[\"tot_class_log - DT subset\"] = a[\"tot_class_log - dt_subset\"]\n",
    "a[\"tot_class_log - DT all\"] = a[\"tot_class_log - dt_all\"]\n",
    "a[\"tot_class_log - RF all\"] = a[\"tot_class_log - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Total time in log scale (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"total_feat_settings_boxplot_log_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "\n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"saved_class - RF subset\"] = a[\"saved_class - rf_subset\"]\n",
    "a[\"saved_class - DT subset\"] = a[\"saved_class - dt_subset\"]\n",
    "a[\"saved_class - DT all\"] = a[\"saved_class - dt_all\"]\n",
    "a[\"saved_class - RF all\"] = a[\"saved_class - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = False);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Saved time with respect to never using tabulation (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"saved_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "    \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class_v_heur - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"saved_class_v_heur - RF subset\"] = a[\"saved_class_v_heur - rf_subset\"]\n",
    "a[\"saved_class_v_heur - DT subset\"] = a[\"saved_class_v_heur - dt_subset\"]\n",
    "a[\"saved_class_v_heur - DT all\"] = a[\"saved_class_v_heur - dt_all\"]\n",
    "a[\"saved_class_v_heur - RF all\"] = a[\"saved_class_v_heur - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = False);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Saved time with respect to the heuristics (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"saved_v_heur_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "             \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_s_best - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"saved_s_best - RF subset\"] = a[\"saved_s_best - rf_subset\"]\n",
    "a[\"saved_s_best - DT subset\"] = a[\"saved_s_best - dt_subset\"]\n",
    "a[\"saved_s_best - DT all\"] = a[\"saved_s_best - dt_all\"]\n",
    "a[\"saved_s_best - RF all\"] = a[\"saved_s_best - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = False);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Saved time with respect to the single best (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"saved_s_best_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_setting(type_folder):\n",
    "    df_solvs = dict()\n",
    "    df_solvs_class = dict()\n",
    "    for tmp_solv in [\"minion\", \"kissat\", \"kissat_mdd\", \"chuffed\"]:\n",
    "        tmp_dir_logs = \"../ML_logs/rf_all/\"+tmp_solv+type_folder\n",
    "        tmp_files = os.listdir(tmp_dir_logs)\n",
    "        tmp_ground_truth = \"../Dataset/results_on_off_\"+tmp_solv+\".csv\"\n",
    "        df_solvs[tmp_solv] = pd.read_csv(tmp_ground_truth)\n",
    "        df_solvs[tmp_solv].drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "        targets = dict()\n",
    "        tot_time_without_tab = 0.0\n",
    "        tot_time_with_tab = 0.0\n",
    "        for prob in df_solvs[tmp_solv][\"Problem\"].unique():\n",
    "            baseline = df_solvs[tmp_solv].query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "            if len(baseline) == 0:\n",
    "                continue\n",
    "            baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "            tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "            if len(tab2) == 0:\n",
    "                continue\n",
    "            tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "            baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "                if model_classifier:\n",
    "                    targets[prob] = {'Target': 0, 'Score': 0.0, 'Tab_t': 3600, 'Base_t': 3600}\n",
    "                else:\n",
    "                    continue  # drop row\n",
    "            if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "                clipped = np.clip(tab2_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "                clipped = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 3600, 'Base_t': clipped}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "                if model_classifier:\n",
    "                    label = 1 if tab2_time < baseline_time else 0\n",
    "                    clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                    clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                    targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "                else:\n",
    "                    continue # Skip small differences\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0:# and np.abs(tab2_time-baseline_time)>=1:\n",
    "                label = 1 if tab2_time < baseline_time else 0\n",
    "                clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "        target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "        target_df['Problem'] = target_df.index\n",
    "        target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)\n",
    "        df_solvs[tmp_solv] = target_df.copy()\n",
    "        name = tmp_dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "        dfs = dict()\n",
    "        if (name == \"instance\" or name == \"problem\") and not gnn_flag:\n",
    "            problems = []\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "                problems.append([test_problems, train_problems])\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "            for split in dfs.keys():\n",
    "                dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"single\" and not gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                complete_df = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test, all_train = pickle.load(pkl_f)\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "                dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"leave\" and not gnn_flag:\n",
    "            complete_df = pd.DataFrame()\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "                #complete_df = pd.concat([complete_df, pd.DataFrame(all_train)])\n",
    "            df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "            dfs['0.1'] = df\n",
    "        elif name == \"instance\" and gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "            for split in dfs.keys():\n",
    "                dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"leave\" and gnn_flag:\n",
    "            complete_df = pd.DataFrame()\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                if len(all_test[\"prediction\"][0][0])==1:\n",
    "                    all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "                complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "            df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "            dfs['0.1'] = df\n",
    "        elif name == \"problem\" and gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "            for split in dfs.keys():\n",
    "                dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"single\" and gnn_flag:\n",
    "            for file in tmp_files:\n",
    "                split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "                if split not in dfs.keys():\n",
    "                    dfs[split] = pd.DataFrame()\n",
    "                complete_df = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "                if len(all_test[\"prediction\"][0][0])==1:\n",
    "                    all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "                dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "                dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "        if name == \"instance\" or name == \"problem\":\n",
    "            for df_key in dfs.keys():\n",
    "                complete_df = dfs[df_key]\n",
    "                df = pd.DataFrame()\n",
    "                num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "                for prob in complete_df['Problem'].unique():\n",
    "                    prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "                    for train_test in prob_df['train_test'].unique():\n",
    "                        a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "                        tmp_df = a[num_cols].mean().to_frame().T\n",
    "                        tmp_df[\"Problem\"] = prob\n",
    "                        tmp_df[\"train_test\"] = train_test\n",
    "                        tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "                        df = pd.concat([df, tmp_df])\n",
    "                dfs[df_key] = df.reset_index().drop([\"index\"], axis=1)\n",
    "                dfs[df_key][\"saved_class_v_heur\"] = dfs[df_key][\"saved_class\"]-dfs[df_key][\"saved_heur\"]\n",
    "            if gnn_flag and \"instance\" in dfs.keys():\n",
    "                dfs['0.3'] = dfs[\"instance\"]\n",
    "            elif gnn_flag and \"problem\" in dfs.keys():\n",
    "                dfs['0.3'] = dfs[\"problem\"]\n",
    "        elif name == \"single\" or name==\"leave\":\n",
    "            for df_key in dfs.keys():\n",
    "                dfs[df_key] = dfs[df_key].groupby([\"Problem\", \"train_test\"]).agg({\n",
    "                    \"saved_v_best\": 'mean',\n",
    "                    \"saved_class\": 'mean',\n",
    "                    \"saved_heur\": 'mean',\n",
    "                    \"tot_best\": 'mean',\n",
    "                    \"tot_class\": 'mean',\n",
    "                    \"tot_heur\": 'mean',\n",
    "                    \"tot_no_tab\": 'mean'\n",
    "                })\n",
    "                dfs[df_key] = dfs[df_key].reset_index()\n",
    "                dfs[df_key][\"saved_class_v_heur\"] = dfs[df_key][\"saved_class\"]-dfs[df_key][\"saved_heur\"]\n",
    "        if type_folder == \"/single_problem_custom\" and not gnn_flag:\n",
    "            keys_dict = {\"minion\": \"0.1\", \"kissat\": \"0.4\", \"kissat_mdd\": \"0.1\", \"chuffed\": \"0.1\"}  # problem class\n",
    "        elif type_folder == \"/leave_one_out_custom\" and not gnn_flag:\n",
    "            keys_dict = {\"minion\": \"0.1\", \"kissat\": \"0.1\", \"kissat_mdd\": \"0.1\", \"chuffed\": \"0.1\"}  # leave_one_out\n",
    "        elif type_folder == \"/instance_custom\" and not gnn_flag:\n",
    "            keys_dict = {\"minion\": \"0.3\", \"kissat\": \"0.5\", \"kissat_mdd\": \"0.2\", \"chuffed\": \"0.3\"}  # instance\n",
    "        if not gnn_flag:\n",
    "            df_solvs_class[tmp_solv] = dfs[keys_dict[tmp_solv]].copy()\n",
    "        else:\n",
    "            df_solvs_class[tmp_solv] = dfs[list(dfs.keys())[0]].copy()\n",
    "    return df_solvs, df_solvs_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of different settings and solvers, against a given setting (no tabulation, heuristics, single best)\n",
    "dfs_boxs = pd.DataFrame()\n",
    "for setting in [\"single_problem\", \"leave_one_out\"]:\n",
    "    type_folder = \"/\"+setting+\"_custom\"\n",
    "    _, df_solvs_class = run_setting(type_folder)\n",
    "    \n",
    "    df_list = []\n",
    "    for tmp_solv in [\"minion\", \"kissat\", \"kissat_mdd\", \"chuffed\"]:\n",
    "        \n",
    "        if setting == \"single_problem\":\n",
    "                df_solvs_class[tmp_solv][\"saved_s_best\"] = 0\n",
    "                for prob in df_solvs_class[tmp_solv].Problem.unique():\n",
    "                    a = df_solvs_class[tmp_solv].query(\"train_test=='train' and Problem=='\"+prob+\"'\")\n",
    "                    mask = df_solvs_class[tmp_solv][\"Problem\"].str.match(prob) & df_solvs_class[tmp_solv][\"train_test\"].str.match(\"train\")\n",
    "                    df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].iloc[0]<=0 else a[\"saved_heur\"]\n",
    "                    b = df_solvs_class[tmp_solv].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                    mask = df_solvs_class[tmp_solv][\"Problem\"].str.match(prob) & df_solvs_class[tmp_solv][\"train_test\"].str.match(\"test\")\n",
    "                    df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].iloc[0]<=0 else b[\"saved_heur\"]\n",
    "                    df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"leave_one_out\":\n",
    "            df_solvs_class[tmp_solv][\"saved_s_best\"] = 0\n",
    "            for prob in df_solvs_class[tmp_solv].Problem.unique():\n",
    "                a = df_solvs_class[tmp_solv].query(\"train_test=='test' and Problem!='\"+prob+\"'\")\n",
    "                b = df_solvs_class[tmp_solv].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                mask = df_solvs_class[tmp_solv][\"Problem\"].str.match(prob) & df_solvs_class[tmp_solv][\"train_test\"].str.match(\"test\")\n",
    "                df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].sum()<=0 else b[\"saved_heur\"]\n",
    "                df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"instance\":\n",
    "            p_list = set([prob[0] for prob in df_solvs_class[tmp_solv][\"Problem\"].str.split(\"_\")])\n",
    "            prob_sum = dict()\n",
    "            for prob in p_list:\n",
    "                prob_sum[prob] = df_solvs_class[tmp_solv][df_solvs_class[tmp_solv][\"Problem\"].str.contains(prob+\"_\")].query(\"train_test=='test'\").sum()\n",
    "            df_solvs_class[tmp_solv] = pd.DataFrame(prob_sum).transpose().drop([\"Problem\", \"prediction\"], axis=1)\n",
    "            df_solvs_class[tmp_solv] = df_solvs_class[tmp_solv].reset_index()\n",
    "            df_solvs_class[tmp_solv][\"Problem\"] = df_solvs_class[tmp_solv][\"index\"]\n",
    "            df_solvs_class[tmp_solv][\"saved_s_best\"] = 0\n",
    "    \n",
    "        if \"prediction\" in df_solvs_class[tmp_solv].columns:\n",
    "            df_list.append(df_solvs_class[tmp_solv].query(\"train_test=='test'\").drop([\"prediction\", \"train_test\"], axis=1).copy().assign(Trial=tmp_solv))\n",
    "        else:\n",
    "            df_list.append(df_solvs_class[tmp_solv].query(\"train_test=='test'\").drop([\"train_test\"], axis=1).copy().assign(Trial=tmp_solv))\n",
    "\n",
    "        if setting == \"instance\":\n",
    "            df_list[-1] = df_list[-1].drop([\"index\"], axis=1).copy().assign(Trial=tmp_solv)\n",
    "            cols = [x for x in df_list[-1].columns if x != \"Trial\" and x!= \"Problem\"]\n",
    "            df_list[-1][cols] = df_list[-1][cols].astype(float)\n",
    "    \n",
    "        tmp_solv_name = tmp_solv[0].upper()+tmp_solv[1:]\n",
    "        df_list[-1][\"saved_class - \"+tmp_solv_name] = df_list[-1][\"saved_class\"]\n",
    "        df_list[-1][\"saved_class_v_heur - \"+tmp_solv_name] = df_list[-1][\"saved_class_v_heur\"]\n",
    "        df_list[-1][\"saved_s_best - \"+tmp_solv_name] = df_list[-1][\"saved_s_best\"]\n",
    "    \n",
    "        df_list[-1] = df_list[-1][[\"saved_class - \"+tmp_solv_name, \"saved_class_v_heur - \"+tmp_solv_name, \n",
    "                                   \"saved_s_best - \"+tmp_solv_name, \"Problem\"]]\n",
    "    \n",
    "    dfs_boxs_tmp = df_list[0].merge(df_list[1], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[2], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[3], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.drop([\"Problem\"], axis=1)\n",
    "    \n",
    "    dfs_boxs_tmp[\"Setting\"] = \" \".join([x[0].upper()+x[1:] for x in setting.split(\"_\")])\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"By Instance\" if x==\"Instance\" else x)\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"Per Problem Class\" if x==\"Single Problem\" else x)\n",
    "    \n",
    "    dfs_boxs = pd.concat([dfs_boxs, dfs_boxs_tmp])\n",
    "    \n",
    "    print(\"Finished \" + setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class - \"+tmp_solv for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]]].copy()\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = True);\n",
    "\n",
    "    plt.title(\"Saved time with respect to never using tabulation\");\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"saved_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "    \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class_v_heur - \"+tmp_solv for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]]].copy()\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = True);\n",
    "\n",
    "    plt.title(\"Saved time with respect to the heuristics\");\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"saved_v_heur_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "             \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_s_best - \"+tmp_solv for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]]].copy()\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = True);\n",
    "\n",
    "    plt.title(\"Saved time with respect to the single best\");\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.savefig(\"saved_s_best_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solvs = dict()\n",
    "df_solvs_class = dict()\n",
    "for tmp_solv in [\"minion\", \"kissat\", \"kissat_mdd\", \"chuffed\"]:\n",
    "    tmp_dir_logs = \"../ML_logs/rf_all/\"+tmp_solv+type_folder\n",
    "    tmp_files = os.listdir(tmp_dir_logs)\n",
    "    tmp_ground_truth = \"../Dataset/results_on_off_\"+tmp_solv+\".csv\"\n",
    "    df_solvs[tmp_solv] = pd.read_csv(tmp_ground_truth)\n",
    "    df_solvs[tmp_solv].drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "    targets = dict()\n",
    "    tot_time_without_tab = 0.0\n",
    "    tot_time_with_tab = 0.0\n",
    "    for prob in df_solvs[tmp_solv][\"Problem\"].unique():\n",
    "        baseline = df_solvs[tmp_solv].query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "        if len(baseline) == 0:\n",
    "            continue\n",
    "        baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "        tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "        if len(tab2) == 0:\n",
    "            continue\n",
    "        tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "        baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "        tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "        if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "            if model_classifier:\n",
    "                targets[prob] = {'Target': 0, 'Score': 0.0, 'Tab_t': 3600, 'Base_t': 3600}\n",
    "            else:\n",
    "                continue  # drop row\n",
    "        if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "            clipped = np.clip(tab2_time, 0, 3600)\n",
    "            targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "        elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "            clipped = np.clip(baseline_time, 0, 3600)\n",
    "            targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 2*3600, 'Base_t': clipped}\n",
    "        elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "            if model_classifier:\n",
    "                label = 1 if tab2_time < baseline_time else 0\n",
    "                clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "            else:\n",
    "                continue # Skip small differences\n",
    "        elif baselineTimeOut==0 and tab2TimeOut==0:# and np.abs(tab2_time-baseline_time)>=1:\n",
    "            label = 1 if tab2_time < baseline_time else 0\n",
    "            clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "            clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "            targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "    target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "    target_df['Problem'] = target_df.index\n",
    "    target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)\n",
    "    df_solvs[tmp_solv] = target_df.copy()\n",
    "    name = tmp_dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "    dfs = dict()\n",
    "    if (name == \"instance\" or name == \"problem\") and not gnn_flag:\n",
    "        problems = []\n",
    "        for file in tmp_files:\n",
    "            split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "            if split not in dfs.keys():\n",
    "                dfs[split] = pd.DataFrame()\n",
    "            filename = os.path.join(tmp_dir_logs, file)\n",
    "            with open(filename, \"rb\") as pkl_f:\n",
    "                test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "            problems.append([test_problems, train_problems])\n",
    "            dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "            dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "        for split in dfs.keys():\n",
    "            dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "    elif name == \"single\" and not gnn_flag:\n",
    "        for file in tmp_files:\n",
    "            split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "            if split not in dfs.keys():\n",
    "                dfs[split] = pd.DataFrame()\n",
    "            complete_df = pd.DataFrame()\n",
    "            filename = os.path.join(tmp_dir_logs, file)\n",
    "            with open(filename, \"rb\") as pkl_f:\n",
    "                all_test, all_train = pickle.load(pkl_f)\n",
    "            dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "            dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_train)])\n",
    "            dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "    elif name == \"leave\" and not gnn_flag:\n",
    "        complete_df = pd.DataFrame()\n",
    "        for file in tmp_files:\n",
    "            filename = os.path.join(tmp_dir_logs, file)\n",
    "            with open(filename, \"rb\") as pkl_f:\n",
    "                all_test = pickle.load(pkl_f)[0]\n",
    "            complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "            #complete_df = pd.concat([complete_df, pd.DataFrame(all_train)])\n",
    "        df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "        dfs['0.1'] = df\n",
    "    elif name == \"instance\" and gnn_flag:\n",
    "        for file in tmp_files:\n",
    "            split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "            if split not in dfs.keys():\n",
    "                dfs[split] = pd.DataFrame()\n",
    "            filename = os.path.join(tmp_dir_logs, file)\n",
    "            with open(filename, \"rb\") as pkl_f:\n",
    "                all_test = pickle.load(pkl_f)[0]\n",
    "            all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "            all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "            dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "        for split in dfs.keys():\n",
    "            dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "    elif name == \"leave\" and gnn_flag:\n",
    "        complete_df = pd.DataFrame()\n",
    "        for file in tmp_files:\n",
    "            filename = os.path.join(tmp_dir_logs, file)\n",
    "            with open(filename, \"rb\") as pkl_f:\n",
    "                all_test = pickle.load(pkl_f)[0]\n",
    "            all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "            if len(all_test[\"prediction\"][0][0])==1:\n",
    "                all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "            complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "        df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "        dfs['0.1'] = df\n",
    "    elif name == \"problem\" and gnn_flag:\n",
    "        for file in tmp_files:\n",
    "            split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "            if split not in dfs.keys():\n",
    "                dfs[split] = pd.DataFrame()\n",
    "            filename = os.path.join(tmp_dir_logs, file)\n",
    "            with open(filename, \"rb\") as pkl_f:\n",
    "                all_test = pickle.load(pkl_f)[0]\n",
    "            all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "            all_test[\"prediction\"] = [0 for x in range(len(all_test[\"Problem\"]))]\n",
    "            dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "        for split in dfs.keys():\n",
    "            dfs[split] = dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "    elif name == \"single\" and gnn_flag:\n",
    "        for file in tmp_files:\n",
    "            split = str(file.split(\"_\")[-1].split(\".pkl\")[0])\n",
    "            if split not in dfs.keys():\n",
    "                dfs[split] = pd.DataFrame()\n",
    "            complete_df = pd.DataFrame()\n",
    "            filename = os.path.join(tmp_dir_logs, file)\n",
    "            with open(filename, \"rb\") as pkl_f:\n",
    "                all_test = pickle.load(pkl_f)[0]\n",
    "            all_test[\"train_test\"] = [\"test\" for x in range(len(all_test[\"Problem\"]))]\n",
    "            if len(all_test[\"prediction\"][0][0])==1:\n",
    "                all_test[\"prediction\"] = [[x[0] for x in all_test[\"prediction\"][i]] for i in range(len(all_test[\"prediction\"]))]\n",
    "            dfs[split] = pd.concat([dfs[split], pd.DataFrame(all_test)])\n",
    "            dfs[split].reset_index().drop([\"index\"], axis=1)\n",
    "    if name == \"instance\" or name == \"problem\":\n",
    "        for df_key in dfs.keys():\n",
    "            complete_df = dfs[df_key]\n",
    "            df = pd.DataFrame()\n",
    "            num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "            for prob in complete_df['Problem'].unique():\n",
    "                prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "                for train_test in prob_df['train_test'].unique():\n",
    "                    a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "                    tmp_df = a[num_cols].mean().to_frame().T\n",
    "                    tmp_df[\"Problem\"] = prob\n",
    "                    tmp_df[\"train_test\"] = train_test\n",
    "                    tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "                    df = pd.concat([df, tmp_df])\n",
    "            dfs[df_key] = df.reset_index().drop([\"index\"], axis=1)\n",
    "        if gnn_flag and \"instance\" in dfs.keys():\n",
    "            dfs['0.3'] = dfs[\"instance\"]\n",
    "        elif gnn_flag and \"problem\" in dfs.keys():\n",
    "            dfs['0.3'] = dfs[\"problem\"]\n",
    "    keys_dict = {\"minion\": \"0.3\", \"kissat\": \"0.2\", \"kissat_mdd\": \"0.5\", \"chuffed\": \"0.2\"}\n",
    "    df_solvs_class[tmp_solv] = dfs[keys_dict[tmp_solv]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_solvs[\"minion\"][\"Best\"] = [x.Tab_t if x.Tab_t<x.Base_t else x.Base_t for x in df_solvs[\"minion\"].itertuples()]\n",
    "df_solvs[\"kissat\"][\"Best\"] = [x.Tab_t if x.Tab_t<x.Base_t else x.Base_t for x in df_solvs[\"kissat\"].itertuples()]\n",
    "df_solvs[\"kissat_mdd\"][\"Best\"] = [x.Tab_t if x.Tab_t<x.Base_t else x.Base_t for x in df_solvs[\"kissat_mdd\"].itertuples()]\n",
    "df_solvs[\"chuffed\"][\"Best\"] = [x.Tab_t if x.Tab_t<x.Base_t else x.Base_t for x in df_solvs[\"chuffed\"].itertuples()]\n",
    "\n",
    "print(\"Total time (s) for each solver with virtual best classifier for tabulation/heuristics: \")\n",
    "print(\"\\t Minion:\", np.sum(df_solvs[\"minion\"][\"Best\"]), \"s\")\n",
    "print(\"\\t Kissat:\", np.sum(df_solvs[\"kissat\"][\"Best\"]), \"s\")\n",
    "print(\"\\t Kissat mdd:\", np.sum(df_solvs[\"kissat_mdd\"][\"Best\"]), \"s\")\n",
    "print(\"\\t Chuffed:\", np.sum(df_solvs[\"chuffed\"][\"Best\"]), \"s\")\n",
    "\n",
    "tmp_sum = 0\n",
    "list_res_notab = dict()\n",
    "for prob in df_solvs[\"minion\"].Problem.unique():\n",
    "    if prob in df_solvs[\"kissat\"].Problem.values and prob in df_solvs[\"kissat_mdd\"].Problem.values and prob in df_solvs[\"chuffed\"].Problem.values:\n",
    "        a = min([df_solvs[\"minion\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0], \n",
    "                 df_solvs[\"kissat\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0], \n",
    "                 df_solvs[\"kissat_mdd\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0], \n",
    "                 df_solvs[\"chuffed\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0]])\n",
    "        tmp_sum += a\n",
    "        list_res_notab[prob] = [a]\n",
    "df_notab = pd.DataFrame.from_dict(list_res_notab).transpose()\n",
    "df_notab[\"Base_t\"] = df_notab[0]\n",
    "df_notab.drop([0], axis=1, inplace=True)\n",
    "print(\"Total time (s) for each solver with virtual best classifier for solver and without tabulation:\", tmp_sum, \"s\")\n",
    "\n",
    "tmp_sum = 0\n",
    "list_res_heur = dict()\n",
    "for prob in df_solvs[\"minion\"].Problem.unique():\n",
    "    if prob in df_solvs[\"kissat\"].Problem.values and prob in df_solvs[\"kissat_mdd\"].Problem.values and prob in df_solvs[\"chuffed\"].Problem.values:\n",
    "        a = min([df_solvs[\"minion\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0], \n",
    "                 df_solvs[\"kissat\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0], \n",
    "                 df_solvs[\"kissat_mdd\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0], \n",
    "                 df_solvs[\"chuffed\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0]])\n",
    "        tmp_sum += a\n",
    "        list_res_heur[prob] = [a]\n",
    "df_heur = pd.DataFrame.from_dict(list_res_heur).transpose()\n",
    "df_heur[\"Tab_t\"] = df_heur[0]\n",
    "df_heur.drop([0], axis=1, inplace=True)\n",
    "print(\"Total time (s) for each solver with virtual best classifier for solver and with the heuristics:\", tmp_sum, \"s\")\n",
    "\n",
    "tmp_sum = 0\n",
    "list_res_class = dict()\n",
    "for prob in df_solvs[\"minion\"].Problem.unique():\n",
    "    if prob in df_solvs[\"kissat\"].Problem.values and prob in df_solvs[\"kissat_mdd\"].Problem.values and prob in df_solvs[\"chuffed\"].Problem.values:\n",
    "        tabs = [df_solvs[\"minion\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0], \n",
    "               df_solvs[\"kissat\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0], \n",
    "               df_solvs[\"kissat_mdd\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0], \n",
    "               df_solvs[\"chuffed\"].query(\"Problem=='\"+prob+\"'\")[\"Tab_t\"].values[0]]\n",
    "        bases = [df_solvs[\"minion\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0], \n",
    "               df_solvs[\"kissat\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0], \n",
    "               df_solvs[\"kissat_mdd\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0], \n",
    "               df_solvs[\"chuffed\"].query(\"Problem=='\"+prob+\"'\")[\"Base_t\"].values[0]]\n",
    "        a = min([tabs[0] if tabs[0]<bases[0] else bases[0], \n",
    "                 tabs[1] if tabs[1]<bases[1] else bases[1], \n",
    "                 tabs[2] if tabs[2]<bases[2] else bases[2], \n",
    "                 tabs[3] if tabs[3]<bases[3] else bases[3]])\n",
    "        tmp_sum += a\n",
    "        list_res_class[prob] = [a]\n",
    "df_class = pd.DataFrame.from_dict(list_res_class).transpose()\n",
    "df_class[\"Best\"] = df_class[0]\n",
    "df_class.drop([0], axis=1, inplace=True)\n",
    "print(\"Total time (s) for each solver with virtual best classifier for solver and for the heuristics:\", tmp_sum, \"s\")\n",
    "\n",
    "df1 = pd.DataFrame([df_solvs[\"minion\"][\"Tab_t\"], df_solvs[\"minion\"][\"Base_t\"], df_solvs[\"minion\"][\"Best\"]]).transpose().assign(Trial=\"Minion\")\n",
    "df2 = pd.DataFrame([df_solvs[\"kissat\"][\"Tab_t\"], df_solvs[\"kissat\"][\"Base_t\"], df_solvs[\"kissat\"][\"Best\"]]).transpose().assign(Trial=\"kissat\")\n",
    "df3 = pd.DataFrame([df_solvs[\"kissat_mdd\"][\"Tab_t\"], df_solvs[\"kissat_mdd\"][\"Base_t\"], df_solvs[\"kissat_mdd\"][\"Best\"]]).transpose().assign(Trial=\"kissat_mdd\")\n",
    "df4 = pd.DataFrame([df_solvs[\"chuffed\"][\"Tab_t\"], df_solvs[\"chuffed\"][\"Base_t\"], df_solvs[\"chuffed\"][\"Best\"]]).transpose().assign(Trial=\"chuffed\")\n",
    "df5 = pd.DataFrame([df_heur[\"Tab_t\"], df_notab[\"Base_t\"], df_class[\"Best\"]]).transpose().assign(Trial=\"v_best_class\")\n",
    "\n",
    "cdf = pd.concat([df1, df2, df3, df4, df5])\n",
    "for col in [\"Tab_t\", \"Base_t\", \"Best\"]:\n",
    "    cdf[col] = np.log(cdf[col])\n",
    "mdf = pd.melt(cdf, id_vars=['Trial'], var_name=['Method'])\n",
    "        \n",
    "if len(mdf) > 1:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    ax = sns.boxplot(x=\"Trial\", y=\"value\", hue=\"Method\", data=mdf, showmeans=True)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.title(\"Total time needed to solve all instances with each solver and virtual best classifier.\");\n",
    "    plt.ylabel(\"Total time in log scale (s).\");\n",
    "    plt.tight_layout();\n",
    "    plt.savefig(\"solver_class_boxplot.jpg\", bbox_inches = 'tight');\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict_prob = dict()\n",
    "problems = list(np.unique([x[0] for x in target_df.Problem.str.split(\"_\")]))\n",
    "for prob in problems:\n",
    "    target_dict_prob[prob] = [np.sum(target_df[target_df.Problem.str.contains(prob+\"_\")][\"Tab_t\"])]\n",
    "target_df_problem = pd.DataFrame.from_dict(target_dict_prob).T\n",
    "target_df_problem[\"Problem\"] = target_df_problem.index\n",
    "target_df_problem[\"Tab_t\"] = target_df_problem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1, 10, 25, 50, 100], [x for x in agreement_dict.values()])\n",
    "plt.title(\"Agreement time for different numbers of tabulated constraints.\")\n",
    "plt.xlabel(\"Thresholds (number of constraints)\")\n",
    "plt.ylabel(\"Agreement time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1, 10, 25, 50, 100], [x for x in disagreement_dict.values()])\n",
    "plt.title(\"Disagreement time for different numbers of tabulated constraints.\")\n",
    "plt.xlabel(\"Thresholds (number of constraints)\")\n",
    "plt.ylabel(\"Disagreement time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in np.array(list(disagreement_dict.values()))+np.array(list(agreement_dict.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1, 10, 25, 50, 100], [x for x in np.array(list(disagreement_dict.values()))+np.array(list(agreement_dict.values()))])\n",
    "plt.title(\"Total time for different numbers of tabulated constraints.\")\n",
    "plt.xlabel(\"Thresholds (number of constraints)\")\n",
    "plt.ylabel(\"Total time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_analyses = False\n",
    "if metric_analyses:\n",
    "    plots = [[], [], [], [], [], []]\n",
    "    for t in [0.1, 0.2, 0.3, 0.5, 0.75, 1.0]:\n",
    "        for p in [1, 30, 60]:\n",
    "            target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=p and row['Base_t']<=p) or np.abs(row['Tab_t'] - row['Base_t']) < t*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)\n",
    "            probs_stats, target_df = get_metrics()\n",
    "            plots[0].append(np.abs(probs_stats[\"fp_time\"]).sum())\n",
    "            plots[1].append(np.abs(probs_stats[\"fn_time\"]).sum())\n",
    "            plots[2].append(np.abs(probs_stats[\"no_diff_time\"]).sum())\n",
    "            plots[3].append(probs_stats[\"precision\"].mean())\n",
    "            plots[4].append(probs_stats[\"recall\"].mean())\n",
    "            plots[5].append(probs_stats[\"f1\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if metric_analyses:\n",
    "    for i in range(len(plots)):\n",
    "        if i>2:\n",
    "            plt.plot(plots[i])\n",
    "    plt.title(\"Precision, recall and f1-score.\")\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    plt.legend([\"precision\", \"recall\", \"f1\"])\n",
    "    plt.show()\n",
    "    plt.plot(np.log(plots[0]))\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    plt.title(\"Total time of false positives.\")\n",
    "    plt.ylabel(\"Log scale, time (s)\")\n",
    "    plt.show()\n",
    "    plt.plot(np.log(plots[1]))\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    plt.title(\"Total time of false negatives.\")\n",
    "    plt.ylabel(\"Log scale, time (s)\")\n",
    "    plt.show()\n",
    "    plt.plot(np.log(plots[2]))\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    plt.title(\"Total time of no diff.\")\n",
    "    plt.ylabel(\"Log scale, time (s)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gnn_flag:\n",
    "    target_df = target_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculateSpeedups, plotOthersScatter\n",
    "featured_paper = [\"bibd\", \"carSequencing\", \"molnars\", \"opd\", \"pegSolitaireState\", \"plotting\"]\n",
    "featured = ['PlottingModelB', 'sokoban']\n",
    "axis_labels = [\"Time without tabulation (s)\", \"Time with classifier (s)\"]\n",
    "base = True\n",
    "df_speedups = calculateSpeedups(target_df, base)\n",
    "plotOthersScatter(target_df, \"others.pdf\", featured, axis_labels, base, \"GNN\", df_speedups)\n",
    "\n",
    "axis_labels = [\"Time when always using the heuristics (s)\", \"Time with classifier (s)\"]\n",
    "base = False\n",
    "df_speedups = calculateSpeedups(target_df, base)\n",
    "plotOthersScatter(target_df, \"others.pdf\", featured, axis_labels, base, \"GNN\", df_speedups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds = 60\n",
    "if not gnn_flag:\n",
    "    titles = {\"saved_class\": \"Time saved for each problem with random forest classifier\", \n",
    "             \"saved_v_best\": \"Time saved for each problem with virtual best classifier\", \n",
    "             \"tot_heur\": \"Total time for each problem when always using the heuristics\", \n",
    "             \"tot_no_tab\": \"Total time for each problem without tabulation.\"}\n",
    "else:\n",
    "    titles = {\"saved_class\": \"Time saved for each problem with Graph Neural Network\", \n",
    "             \"saved_v_best\": \"Time saved for each problem with virtual best classifier\", \n",
    "             \"tot_heur\": \"Total time for each problem when always using the heuristics\", \n",
    "             \"tot_no_tab\": \"Total time for each problem without tabulation.\"}\n",
    "saved_thresh = \", when the time saved is more than \"+str(seconds)+\" seconds.\"\n",
    "total_thresh = \", when the total time is more than \"+str(seconds)+\" seconds.\"\n",
    "for col in [\"saved_class\", \"saved_v_best\", \"tot_heur\", \"tot_no_tab\"]:\n",
    "    for ext in [\".\", \"thresh\"]:\n",
    "        if ext == \"thresh\" and col.split(\"_\")[0] == \"saved\":\n",
    "            title = titles[col] + saved_thresh\n",
    "            thresh = seconds\n",
    "        elif ext == \"thresh\" and col.split(\"_\")[0] == \"tot\":\n",
    "            title = titles[col] + total_thresh\n",
    "            thresh = seconds\n",
    "        elif ext == \".\":\n",
    "            title = titles[col] + \".\"\n",
    "            thresh = 0\n",
    "        a = dfs[best_key].query(\"train_test == 'test'\")[[\"Problem\", col]]\n",
    "        a = a[a[col]>thresh]\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        plt.bar(np.arange(0, len(a[\"Problem\"]), 1), np.log(a[col]))\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Problem\")\n",
    "        plt.ylabel(\"Time (s), logaritmic scale\")\n",
    "        plt.xticks(ticks=np.arange(0, len(a[\"Problem\"]), 1), labels=a[\"Problem\"]);\n",
    "        plt.savefig(col+\"_\"+ext+\".png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(len(dfs[best_key].query(\"train_test == 'test'\")), 2, figsize=(12,200))\n",
    "cont = 0\n",
    "for _, row in dfs[best_key].query(\"train_test == 'test'\").iterrows():\n",
    "    a = row.drop([\"Problem\", \"train_test\", \"prediction\"], axis = 0)\n",
    "    b = a[['saved_v_best', 'saved_class', 'saved_heur']]\n",
    "    c = a[['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab']]\n",
    "    axis[cont, 0].bar([0, 1, 2], b)\n",
    "    axis[cont, 0].set_title(\"Total time saved for problem \"+row[\"Problem\"])\n",
    "    axis[cont, 0].set_xticks(ticks=np.arange(0,len(b),1), labels=['saved_v_best', 'saved_class', 'saved_heur'])\n",
    "    \n",
    "    axis[cont, 1].bar([0, 1, 2, 3], c)\n",
    "    axis[cont, 1].set_title(\"Total time for problem \"+row[\"Problem\"])\n",
    "    axis[cont, 1].set_xticks(ticks=np.arange(0,len(c),1), labels=['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab'])\n",
    "    cont+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "for _, row in dfs[best_key].query(\"train_test == 'test'\").iterrows():\n",
    "    if row[\"tot_heur\"] >= row[\"tot_no_tab\"]:\n",
    "        cont+=1\n",
    "figure, axis = plt.subplots(cont, 1, figsize=(12,80))\n",
    "cont = 0\n",
    "all_list = {}\n",
    "for _, row in dfs[best_key].query(\"train_test == 'test'\").iterrows():\n",
    "    a = row.drop([\"Problem\", \"train_test\", \"prediction\"], axis = 0)\n",
    "    if a[\"tot_heur\"] < a[\"tot_no_tab\"]:\n",
    "        continue\n",
    "    b = a[['saved_v_best', 'saved_class', 'saved_heur']]\n",
    "    c = a[['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab']]\n",
    "    \n",
    "    axis[cont].bar([0, 1, 2, 3], c)\n",
    "    axis[cont].set_title(\"Total time for problem \"+row[\"Problem\"])\n",
    "    axis[cont].set_xticks(ticks=np.arange(0,len(c),1), labels=['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab'])\n",
    "    cont+=1\n",
    "    all_list[row[\"Problem\"]] = a[\"tot_heur\"] - a[\"tot_no_tab\"]\n",
    "print(all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_key in dfs.keys():\n",
    "    print(\"Split: \", df_key, \"\\n\", dfs[df_key].drop([\"Problem\", \"train_test\"], axis = 1).describe(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not gnn_flag:\n",
    "    for df_key in dfs:\n",
    "        df = dfs[df_key]\n",
    "        plt.figure(figsize=(8, 8));\n",
    "        ax = plt.gca()\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            max_time = int(max(df.query(\"train_test=='train'\")['tot_best'].max(), df.query(\"train_test=='train'\")['tot_heur'].max()))\n",
    "        ax.scatter(df.query(\"train_test=='train'\")['tot_heur'], df.query(\"train_test=='train'\")['tot_best'], c=\"black\", marker=\"x\");\n",
    "        ax.set_yscale('symlog')\n",
    "        ax.set_xscale('symlog')\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "        else:\n",
    "            plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "        if name != 'leave':\n",
    "            plt.title(\"Total time for problems in train set with split=\"+df_key);\n",
    "            plt.ylabel(\"Total time of virtual best classifier (s) with split=\"+df_key);\n",
    "            plt.xlabel(\"Total time when always using the heuristics (s) with split=\"+df_key);\n",
    "        else:\n",
    "            plt.title(\"Total time for problems in train set.\");\n",
    "            plt.ylabel(\"Total time of virtual best classifier (s).\");\n",
    "            plt.xlabel(\"Total time when always using the heuristics (s).\");\n",
    "        plt.tight_layout();\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            plt.xlim(left=0, right=max_time);\n",
    "            plt.ylim(bottom=0, top=max_time);\n",
    "        else:\n",
    "            plt.xlim(left=0, right=4800);\n",
    "            plt.ylim(bottom=0, top=4800);\n",
    "        #plt.savefig(\"best_heuristics_\"+df_key+\".jpg\", bbox_inches = 'tight')\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_key in [best_key]:  # dfs.keys():\n",
    "    df = dfs[df_key]\n",
    "    plt.figure(figsize=(6, 6));\n",
    "    ax = plt.gca()\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        max_time = int(max(df.query(\"train_test=='test'\")['tot_best'].max(), df.query(\"train_test=='test'\")['tot_class'].max()))\n",
    "    ax.scatter(df.query(\"train_test=='test'\")['tot_best'], df.query(\"train_test=='test'\")['tot_class'], c=\"black\", marker=\"x\");\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.set_xscale('symlog')\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "    else:\n",
    "        plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "    if name != 'leave':\n",
    "        plt.title(\"Total time for problems in test set with split=\"+df_key);\n",
    "        plt.ylabel(\"Total time of random forest classifier (s) with split=\"+df_key);\n",
    "        plt.xlabel(\"Total time of virtual best classifier (s) with split=\"+df_key);\n",
    "    else:\n",
    "        plt.title(\"Total time for problems in test set.\");\n",
    "        plt.ylabel(\"Total time of random forest classifier (s).\");\n",
    "        plt.xlabel(\"Total time of virtual best classifier (s).\");\n",
    "    plt.tight_layout();\n",
    "    if name == \"leave\" or name == \"single\":\n",
    "        plt.xlim(left=0, right=max_time);\n",
    "        plt.ylim(bottom=0, top=max_time);\n",
    "    else:\n",
    "        plt.xlim(left=0, right=4800);\n",
    "        plt.ylim(bottom=0, top=4800);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not gnn_flag:\n",
    "    for df_key in dfs:\n",
    "        df = dfs[df_key]\n",
    "        plt.figure(figsize=(8, 8));\n",
    "        ax = plt.gca()\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            max_time = int(max(df.query(\"train_test=='train'\")['tot_best'].max(), df.query(\"train_test=='train'\")['tot_class'].max()))\n",
    "        ax.scatter(df.query(\"train_test=='train'\")['tot_best'], df.query(\"train_test=='train'\")['tot_class'], c=\"black\", marker=\"x\");\n",
    "        ax.set_yscale('symlog')\n",
    "        ax.set_xscale('symlog')\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "        else:\n",
    "            plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "        if name != 'leave':\n",
    "            plt.title(\"Total time for problems in train set with split=\"+df_key);\n",
    "            plt.ylabel(\"Total time of random forest classifier (s) with split=\"+df_key);\n",
    "            plt.xlabel(\"Total time of virtual best classifier (s) with split=\"+df_key);\n",
    "        else:\n",
    "            plt.title(\"Total time for problems in train set.\");\n",
    "            plt.ylabel(\"Total time of random forest classifier (s).\");\n",
    "            plt.xlabel(\"Total time of virtual best classifier (s).\");\n",
    "        plt.tight_layout();\n",
    "        if name == \"leave\" or name == \"single\":\n",
    "            plt.xlim(left=0, right=max_time);\n",
    "            plt.ylim(bottom=0, top=max_time);\n",
    "        else:\n",
    "            plt.xlim(left=0, right=4800);\n",
    "            plt.ylim(bottom=0, top=4800);\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df_key in dfs:\n",
    "    df = dfs[df_key]\n",
    "    print(df_key,\": \",df.query(\"train_test=='test'\")[df.query(\"train_test=='test'\")['tot_heur']+1< df.query(\"train_test=='test'\")['tot_class']].Problem.apply(lambda x: x.split(\"_\")[0]).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not gnn_flag:\n",
    "    for df_key in dfs:\n",
    "        df = dfs[df_key]\n",
    "        print(df_key,\": \",df.query(\"train_test=='train'\")[df.query(\"train_test=='train'\")['tot_heur']+1< df.query(\"train_test=='train'\")['tot_class']].Problem.apply(lambda x: x.split(\"_\")[0]).unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
